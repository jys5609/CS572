@article{Anina2015,
author = {Anina, Iryna and Zhou, Ziheng and Zhao, Guoying and Pietikainen, Matti},
doi = {10.1109/FG.2015.7163155},
file = {:home/lonbak/study/CS572/literature/paper/07163155.pdf:pdf},
isbn = {9781479960262},
journal = {2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition, FG 2015},
title = {{OuluVS2: A multi-view audiovisual database for non-rigid mouth motion analysis}},
year = {2015}
}
@article{Bowden2013,
abstract = {Human lip-readers are increasingly being presented as useful in the gathering of forensic evidence but, like all humans, suffer from unreliability. Here we report the results of a long-term study in automatic lip-reading with the objective of converting video-to-text (V2T). The V2T problem is surprising in that some aspects that look tricky, such as real-time tracking of the lips on poor-quality interlaced video from hand-held cameras, but prove to be relatively tractable. Whereas the problem of speaker independent lip-reading is very demanding due to unpredictable variations between people. Here we review the problem of automatic lip-reading for crime fighting and identify the critical parts of the problem.},
author = {Bowden, Richard and Cox, Stephen and Harvey, Richard and Lan, Yuxuan and Ong, Eng-Jon and Owen, Gari and Theobald, Barry-John},
doi = {10.1117/12.2029464},
file = {:home/lonbak/study/CS572/literature/paper/spie2013.pdf:pdf},
isbn = {9780819497703},
issn = {08910138},
journal = {Proceedings of SPIE},
keywords = {lip-reading,pattern recognition,speech recognition},
number = {November 2015},
pages = {89010J--89010J--13},
title = {{Recent developments in automated lip-reading}},
url = {http://dx.doi.org/10.1117/12.2029464},
year = {2013}
}
@article{Bt2010,
author = {Bt, Belfast},
file = {:home/lonbak/study/CS572/literature/paper/05650963.pdf:pdf},
isbn = {9781424479931},
journal = {Electrical Engineering},
pages = {2417--2420},
title = {{AN INVESTIGATION INTO FEATURES FOR MULTI-VIEW LIPREADING}},
year = {2010}
}
@article{Lee,
author = {Lee, Daehyun and Lee, Jongmin and Kim, Kee-eung},
file = {:home/lonbak/study/CS572/literature/paper/accv2016final{\_}dhlee.pdf:pdf},
pages = {1--14},
title = {{Multi-View Automatic Lip-Reading using Neural Network}}
}
@article{Li2008,
abstract = {In a lip-reading system, one key issue is how to extract the visual features, which greatly impact on the lip-reading recognition accuracy and efficiency. In this paper, we propose a novel motion based visual feature representation. Compared with the existing methods, our approach focuses on the crucial part of lip movement, but not all pixels around lip contours for different utterance, and captures the motion tracks of each part. Accordingly, distinctive feature vectors are built to represent the whole lip motion process for the specified utterance, rather than the separate frame images. Experimental result shows the efficacy of the proposed approach.},
author = {Li, Meng and Cheung, Yiu Ming},
doi = {10.1109/CIS.2008.214},
file = {:home/lonbak/study/CS572/literature/paper/04724674.pdf:pdf},
isbn = {9780769535081},
journal = {Proceedings - 2008 International Conference on Computational Intelligence and Security, CIS 2008},
pages = {361--365},
title = {{A novel motion based lip feature extraction for lip-reading}},
volume = {1},
year = {2008}
}
@article{Ngiam2011,
abstract = {Deep networks have been successfully applied to unsupervised feature learning for single modalities (e.g., text, images or audio). In this work, we propose a novel application of deep networks to learn features over multiple modalities. We present a series of tasks for multimodal learning and show how to train deep networks that learn features to address these tasks. In particular, we demonstrate cross modality feature learning, where better features for one modality (e.g., video) can be learned ifmultiple modalities (e.g., audio and video) are present at feature learning time. Furthermore, we show how to learn a shared representation between modalities and evalu- ate it on a unique task, where the classifier is trained with audio-only data but tested with video-only data and vice-versa. Our mod- els are validated on the CUAVE and AVLet- ters datasets on audio-visual speech classifi- cation, demonstrating best published visual speech classification on AVLetters and effec- tive shared representation learning.},
annote = {Use audio to train and video for testing, and opportite},
archivePrefix = {arXiv},
arxivId = {1502.07209},
author = {Ngiam, Jiquan and Khosla, Aditya and Kim, Mingyu and Nam, Juhan and Lee, Honglak and Ng, Andrew Y},
doi = {10.1145/2647868.2654931},
eprint = {1502.07209},
file = {:home/lonbak/study/CS572/literature/paper/icml11-MultimodalDeepLearning.pdf:pdf},
isbn = {9781450306195},
journal = {Proceedings of The 28th International Conference on Machine Learning (ICML)},
pages = {689--696},
title = {{Multimodal Deep Learning}},
year = {2011}
}
@article{Potamianos2003,
author = {Potamianos, Gerasimos and Neti, Chalapathy},
file = {:home/lonbak/study/CS572/literature/paper/f3ed2d272d592914ce5d51a0ea03a1b092bf.pdf:pdf},
journal = {Interspeech},
pages = {1293--1296},
title = {{Audio-visual speech recognition in challenging environments.}},
url = {http://www.tsi.enst.fr/{~}chollet/Biblio/Articles/Domaines/BIOMET/AudioVisual/Old/EURO03{\_}CHALLENGING.pdf},
year = {2003}
}
@article{Shaikh2010,
abstract = {This paper presents a lip reading technique to classify the discrete utterances without evaluating the acoustic signals. The reported technique analysis the video data of lip motions by computing the optical flow (OF). The statistical properties of the vertical OF component were used to form the feature vectors for training the support vector machines (SVM) classifier. The impact of the variation in speed/velocity of speaking on the performance of the system was minimized by removing the zero energy frames and normalizing the number of frames by interpolation. The resulting system is an efficient visual viseme classifier with high accuracy (95.9{\%}), specificity (98.1{\%}) and sensitivity (66.4{\%}). The results of the experiments demonstrate the developed technique is insensitive to inter speaker variations.},
author = {Shaikh, Ayaz A and Kumar, Dinesh K and Yau, Wai C and Gubbi, Jayavardhana},
doi = {10.1109/CISP.2010.5646264},
file = {:home/lonbak/study/CS572/literature/paper/2010LipReadingCISP2010.pdf:pdf},
isbn = {9781424465163},
journal = {3rd International Congress on Image and Signal Processing},
keywords = {lipreading,optical flow,support vector machine},
pages = {327--330},
title = {{Lip reading using optical flow and support vector machines}},
url = {http://ieeexplore.ieee.org/xpls/abs{\_}all.jsp?arnumber=5646264},
year = {2010}
}
@article{Summerfield1992,
abstract = {This paper reviews progress in understanding the psychology of lipreading and audio-visual speech perception. It considers four questions. What distinguishes better from poorer lipreaders? What are the effects of introducing a delay between the acoustical and optical speech signals? What have attempts to produce computer animations of talking faces contributed to our understanding of the visual cues that distinguish consonants and vowels? Finally, how should the process of audio-visual integration in speech perception be described; that is, how are the sights and sounds of talking faces represented at their conflux?},
author = {Summerfield, Quentin},
doi = {10.1098/rstb.1992.0009},
file = {:home/lonbak/study/CS572/literature/paper/55477.pdf:pdf},
issn = {0962-8436},
journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
number = {1273},
pages = {71--78},
title = {{Lipreading and Audio-Visual Speech Perception}},
volume = {335},
year = {1992}
}
@inproceedings{Wand2016,
author = {Wand, Michael and Koutnik, Jan and Schmidhuber, Jurgen},
booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
doi = {10.1109/ICASSP.2016.7472852},
file = {:home/lonbak/study/CS572/literature/paper/1601.08188v1.pdf:pdf},
isbn = {978-1-4799-9988-0},
month = {mar},
pages = {6115--6119},
publisher = {IEEE},
title = {{Lipreading with long short-term memory}},
url = {http://ieeexplore.ieee.org/document/7472852/},
year = {2016}
}
@article{Krizhevsky2012,
abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSRVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5{\%} and 17.0{\%} which is considerably better than the previous state of the art. The neural network, which has 60 million paramters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolutional operation. To reduce overfitting in the fully-connected layers, we employed a recently-developed method called 'dropout' that proved to be effective. We also entered a variant of the model in the ILSVRC-2012 competition and achievd a top-5 test error rate of 15.3{\%}, compared to 26.2{\%} achieved by the second-best entry.},
archivePrefix = {arXiv},
arxivId = {1102.0183},
author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
doi = {http://dx.doi.org/10.1016/j.protcy.2014.09.007},
eprint = {1102.0183},
file = {:home/lonbak/study/CS572/literature/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf:pdf},
isbn = {9781627480031},
issn = {10495258},
journal = {Advances In Neural Information Processing Systems},
pages = {1--9},
pmid = {7491034},
title = {{ImageNet Classification with Deep Convolutional Neural Networks}},
year = {2012}
}
@article{Lucas1981,
abstract = {Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system.},
archivePrefix = {arXiv},
arxivId = {3629719},
author = {Lucas, Bruce D and Kanade, Takeo},
doi = {10.1109/HPDC.2004.1323531},
eprint = {3629719},
file = {:home/lonbak/study/CS572/literature/paper/lucaskanade81.pdf:pdf},
isbn = {0769521754},
issn = {17486815},
journal = {Imaging},
number = {x},
pages = {674--679},
pmid = {16140533},
title = {{An Iterative Image Registration Technique with an Application to Stereo Vision}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.49.2019{\&}rep=rep1{\&}type=pdf},
volume = {130},
year = {1981}
}
