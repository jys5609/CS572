% As a general rule, do not put math, special symbols or citations
% in the abstract
\begin{abstract}
Auto lip-reading (ALR) is a promising method for enhancing speech recognition, by combining the audio input with a visual.
ALR architectures is usually composed of three parts, feature extraction, temporal correlation of the features and classification.
The feature extraction part is here often complicated by large variety in mouth shape and visual pronunciation. 
In this work we propose a new architecture for ALR, together with a way for augmenting the training data of a multi-view setting.
The architecture is then evaluated in a single-view and multi-view setting. 
With the proposed architecture an accuracy of 81.4\% for an single-view setting and 90.0\% for a multi-view, is obtained.
Both of these is outperforming the current state-of-the-art architecture with 3.4\% and 10\% respectively.
\end{abstract}
