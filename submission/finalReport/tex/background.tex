\section{Background}
\subsection{Convolutional Neural Network}

Convolutional neural networks (CNN) is usually used in many fields of computer vision. CNN is classically applied to classification and detection task.
CNN is inspired by multi-layer perceptron containing small sub-regions of a visual field called receptive field [16]. 
Unlike fully connected layered network, CNN has sparse connectivity and shared weights for the purpose
of increasing computational efficiency and global representation power. CNN is now the most popular and effective selection for learning visual features in computer vision and machine learning fields. We obtain a feature map at layer h with input x pixel at coordinates (i, j) as the following equation:
\begin{equation}
h_{ij} = a((W \cdot x)_{ij}+b)
\end{equation}

, where weight matrix W and bias vector b is the filter of this feature map, a is activation function for non-linearities.
    
\subsection{Long Short-Term Memory}

Among numerous methodologies, recurrent neural network (RNN) and its variants are now common in handling sequential data with their promise of performance and ease of use. The fundamental neural network does not consider the dependency of all inputs and outputs. However, in various tasks, there exist dependency in inputs and outputs, such as sentence analysis. RNN recurrently use the previous computation result to compute the current output. Long-short term memory (LSTM) \cite{hochreiter1997long}, one of the most popular RNN variants that is able to capture long-range dependencies, is commonly adopted. LSTM is RNN with gates, which is proposed to prevent the vanishing gradient problem, becoming more effective in dealing with long sequences.

The basic structure of LSTM unit consists of a cell state with three essential gates: input gate, forget gate and output gate. The cell controls the information storing for a long period via gates. Given an input vector $\mathbf x_t$ at time step $t$, the formal equation for updating gates, output and cell state are defined as follows:
$$\mathbf i_t=\sigma\left({\mathbf x}_{t}{\mathbf U}^{i} + {\mathbf 
	h}_{t-1}{\mathbf W}^{i}\right)$$
$$\mathbf f_t=\sigma\left({\mathbf x}_{t}{\mathbf U}^{f} + {\mathbf 
	h}_{t-1}{\mathbf W}^{f}\right)$$
$$\mathbf o_t=\sigma\left({\mathbf x}_{t}{\mathbf U}^{o} + {\mathbf 
	h}_{t-1}{\mathbf W}^{o}\right)$$
$$\mathbf c_t=\mathbf c_{t-1}\circ \mathbf f_t + \mathbf i_t\circ 
\tanh\left({\mathbf x}_{t}{\mathbf U}^{c} + {\mathbf h}_{t-1}{\mathbf 
	W}^{c}\right)$$
$$\mathbf h_t=\tanh\left(c_t\right)\circ \mathbf o_t$$

where $\mathbf{W}^i, \mathbf{W}^f, \mathbf{W}^o, \mathbf{W}^c \in \mathbb{R}^{N \times N}$, $\mathbf{U}^i, \mathbf{U}^f, \mathbf{U}^o, \mathbf{U}^c \in \mathbb{R}^{N \times N}$ are weight matrices, $\mathbf h_t$ is output vector and $i, f$ and $o$ represent input ($i$), forget ($f$) and output ($o$) gates.

\subsection{Bi-directional LSTM}

The object by using LSTM is to predict state of current time step with previous history. To consider the previous time steps is helpful to improve the performance. However, in the analysis of sequential data, the state of next time step is also important data for predicting the current state. For example, letâ€™s think about the sentence. In the sentence, we know that noun can be located after the adjective or article. The other way, we know that the adjective and article can be located before the noun. Likewise, in the lip reading task, the important thing for analysis of current time step is not only the previous shape of mouth but also next shape of mouth. Thus we applied the bi-directional LSTM for considering the forward and backward sequence. 


We are planning to apply the bi-directional LSTM, which considers the both directional(forward and backward) sequence of data. There are some reseach about outperforming result of bi-directional LSTM(\cite{BiLSTM}). 


\subsection{Batch Normalization}
Neural network is usually trained with mini-batches. Batch Normalization performs normalizing, scaling and shifting the values of mini-batch. The overall algorithm is following: Batch Normalization has two kind of input. One is values of $x$ over a mini-batch: $B = {x_1,x_2,\dots,x_m};$, and the other is parameters to be learned: $\gamma, \beta$. It's output has following equations ${y_i = BN_{\gamma,\beta}(x_i)}$\\
$$ \mu_B \leftarrow \frac{1}{m}\sum_{i=1}^{m}x_i   \quad   \sigma^2_B \leftarrow \frac{1}{m}\sum_{i=1}^{m}(x_i-\mu_B)^2$$
$$\hat{x_i} \leftarrow \frac{x_i - \mu_B}{\sqrt{\sigma^2_B + \epsilon}}$$
$$y_i \leftarrow \gamma\hat{x_i} + \beta $$



